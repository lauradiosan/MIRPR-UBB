{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Automatic Image Processing\n",
    "\n",
    "Classic approach\n",
    "- Aim? => Image enhancement\n",
    "- How? => by using various filters \n",
    "\n",
    "ML-based approaches\n",
    "- Aim? => Image classification and recognition \n",
    "- How?\n",
    "    * Feature extraction & ML algorithms\n",
    "        - Features: Haar, HOG, SIFT, SURF, LBP\n",
    "        - ML algorithms: kNN, SVM, Decision trees and Ada boost\n",
    "    * Feature learning & ML algorithms\n",
    "        - Use ANN for both feature extraction and ML\n",
    "\n",
    "\n",
    "\n",
    "Various recognition tasks:\n",
    "- image classiifcation <img src=\"images/imgClassification.png\" alt=\"classification\" width=\"300\"/>\n",
    "\n",
    "- object detection <img src=\"images/others.png\" alt=\"detection\" width=\"400\"/>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Image classification  <img src=\"images/imgClassif.png\" alt=\"classification\" width=\"400\"/>\n",
    "\n",
    "Input\n",
    "- More labeled images (for training)\n",
    "- More (n) images (for testing – without labels)\n",
    "\n",
    "Output\n",
    "- Label associated to input images\n",
    "\n",
    "Evaluation \n",
    "- Datasets – image classification task form\n",
    "    * MNIST\n",
    "    * CIFAR\n",
    "    * Pascal VOC http://host.robots.ox.ac.uk/pascal/VOC/\n",
    "        - 2005 – image classification task (4 classes, 1578 images, 2209 objects)\n",
    "        - 2006 – image classification task (10 classes, 2618 images, 4754 objects)\n",
    "        - …\n",
    "        - 2012 – image classification task (20 classes, 11 530 images, 6929 objects)\n",
    "\n",
    "    * ImageNet http://www.image-net.org/\n",
    "        - 2010 – image classification task only (1000 classes, 14,197,122 images, )\n",
    "        - 2011, … - other tasks (localisation, segmentation, detection)\n",
    "\n",
    "Metrics \n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- AUC\n",
    "\n",
    "How?\n",
    "- Features + ML algorithm\n",
    "    * Features: histograms, HOG, Bag of words, …\n",
    "    * ML algorithm: Decision trees, SVM, ANN\n",
    "- ML algorithm (that processes the raw images)\n",
    "    * kNN\n",
    "    * ANN \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Image classification = \"manual\" feature extraction + ML algorithm\n",
    "\n",
    "Why features?\n",
    "- Features can transform a non-linear problem into a linear one <img src=\"images/kernelTrick.png\" alt=\"classification\" width=\"300\"/>\n",
    "\n",
    "\n",
    "What features?\n",
    "- Histogram of colours (Gray, RGB) <img src=\"images/histoEx.png\" alt=\"classification\" width=\"400\"/>\n",
    "    * + reducing input size\n",
    "    * - loosing localization \n",
    "\n",
    "- Histogram of oriented gradients  <img src=\"images/hogEx.png\" alt=\"classification\" width=\"400\"/>\n",
    "    * + preserved localization \n",
    "    * - sensitive to image rotation \n",
    "\n",
    "- Bag of words - see [material](http://www.micc.unifi.it/delbimbo/wp-content/uploads/2011/10/slide_corso/A31_bag_of_visual_words_representation.pdf) <img src=\"images/bow.png\" alt=\"classification\" width=\"400\"/>\n",
    "    * + deals well with occlusion, scale invariant, rotation invariant \n",
    "    * - efficiency of generating the vocabulary, spatial relationship among patches \n",
    "\n",
    "What ML algorithm (that processes the raw images)?\n",
    "- kNN\n",
    "    * Train\n",
    "        - Memorize all data and labels \n",
    "        - O(1)\n",
    "    * Test \n",
    "        - Predict the label of the most k similar training image\n",
    "        - O(n) –  too slow!!!\n",
    "    * Hyperparameters – optimised on a validation dataset\n",
    "        - k (no of neighbours)\n",
    "        - Similarity metric \n",
    "            * L1 (Manhattan) distance $ d(I_1, I_2) = \\sum_{p}{|I_1^p - I_2^p|}$\n",
    "\n",
    "            <img src=\"images/manhattan.png\" alt=\"classification\" width=\"400\"/>\n",
    "\n",
    "            * L2 (Euclidean) distance $ d(I_1, I_2) = \\sqrt{\\sum_{p}{(I_1^p - I_2^p)^2}}$\n",
    "\n",
    "\n",
    "- ANN  <img src=\"images/annImgClassif.png\" alt=\"classification\" width=\"400\"/>\n",
    "\n",
    "    * Encodes parameters W\n",
    "    * Train\n",
    "        - Optimise the parameters W based on training data \n",
    "            * W – net’s architecture\n",
    "            * Loss function\n",
    "        - O(|W| * n * noEpochs)\n",
    "    * Test \n",
    "        - Use the learnt parameters to predict the label \n",
    "    * Hyperparameters \n",
    "        - net’s architecture\n",
    "        - Learning rate \n",
    "        - … \n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Image classification = ML algorithm\n",
    "\n",
    "A single ANN extract features and solve the classification problem\n",
    "\n",
    "Common architectures:\n",
    "- Classical CNNs\n",
    "    * LeNet (1998)\n",
    "        - proposed by Yann LeCun, 1998 - [paper](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "        - architecture: a conv layer + a pool layer\n",
    "    * AlexNet (2012) – first Deep CNN\n",
    "        - proposed by lex Krizhevsky, Ilya Sutskever and Geoff Hinton, 2012 [paper](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "        - architecture:  More conv layers + more pool layers\n",
    "    * ZfNet (2013)\n",
    "        - proposed by Matthew Zeiler and Rob Fergus, 2013 [paper](https://arxiv.org/pdf/1311.2901.pdf)\n",
    "        - architecture: AlexNet + optimisation of hyper-parameters\n",
    "- Modern CNNs\n",
    "    * VGG (2014)\n",
    "        - proposed by Karen Simonyan and Andrew Zisserman, 2014 [paper](https://arxiv.org/pdf/1409.1556.pdf)\n",
    "        - architecture: 16 Conv/FC layers (FC -> a lot more memory; they can be eliminated)\n",
    "\n",
    "    * NiN (2014)\n",
    "    * GoogleLeNet (2014)\n",
    "        - propsoed by Christian Szegedy et al., 2014 [paper](https://arxiv.org/pdf/1409.4842.pdf)\n",
    "        - architecture:\n",
    "            * Inception Module that dramatically reduced the number of parameters in the network (AlexNet 60M, GoogleLeNet 4M) [detalis](https://arxiv.org/pdf/1602.07261.pdf)\n",
    "            * uses Average Pooling instead of Fully Connected layers at the top of the ConvNet => eliminating parameters\n",
    "    * MobileNet (2017)\n",
    "    * ResNet (2015)\n",
    "        - proposed by Kaiming He et al., 2015 [paper](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "        - architecture:\n",
    "            * skip connections \n",
    "            * batch normalization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "# LeNet (1998)\n",
    "\n",
    "Parent\n",
    "- Yann LeCun (NYU), MNIST data\n",
    "- LeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. “Gradient-Based Learning Applied to Document Recognition.” In Proceedings of the IEEE, 2278–2324 http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf\n",
    "\n",
    "Flow:\n",
    "- Input -> 2 x \\[Conv -> Pool\\] -> FC -> FC -> softmax -> Output (10 classes)\n",
    "\n",
    " <img src=\"images/leNet.png\" alt=\"classification\" width=\"600\"/>\n",
    "\n",
    "\n",
    "Input:\n",
    "- Grayscale image 28 x 28\n",
    "\n",
    "Activation:\n",
    "- Tahn\n",
    "\n",
    "    * 0 centered: on avg., values -> 0 => derivative -> 1\n",
    "\n",
    "- Sigm\n",
    "\n",
    "    * 0.5 centered: on avg., values -> 0.5 => derivative -> ~0.25 < 1\n",
    "\n",
    "<img src=\"images/activations.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n",
    "- Issues :\n",
    "    \n",
    "    * vanishing gradient problem (VGP = the gradient of the activation becomes negligible)\n",
    "\n",
    "\n",
    "Filters (#filters(size, padding, stride)\n",
    "- 6(5 x 5, 2, 1), 16(5 x 5, 0, 1)\n",
    "\n",
    "Pooling\n",
    "- Avg-pooling 2x2, stride 2\n",
    "\n",
    "Loss\n",
    "- Softmax (cross-entropy)\n",
    "\n",
    "#parameters\n",
    "- 60 000\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# AlexNet (2012)\n",
    "\n",
    "Parents\n",
    "- Alex Krizhevsky et al., ImageNet data\n",
    "- Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1, 1097–1105. NIPS’12. Lake Tahoe, Nevada [link](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "- Winner of ILSVRC 2012 (5-6 days for training - GTX 580 GPUs)\n",
    "\n",
    "Flow:\n",
    "- Input -> 2 x \\[Conv -> Pool -> Norm\\] -> 3 x Conv -> Pool -> 3 x \\[FC -> DropOut\\] -> Softmax -> Output (1000 classes)\n",
    "\n",
    "<img src=\"images/alexNet.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n",
    "Input\n",
    "- RGB image 224 x 224 x 3\n",
    "\n",
    "Activation\n",
    "- ReLU, sigmoid\n",
    "- Conv -> ReLU\n",
    "    * Advantages\n",
    "        - Feature sparsity\n",
    "        - Reducing VGP\n",
    "    * Drawbacks\n",
    "        - Dying ReLU problem: output(node) < 0 => derivative  = 0 => weights are not changed / trained\n",
    "        - Conv layers are more affected by VGP\n",
    "- FC -> tahn\n",
    "    * FC layers are less affected by VGP\n",
    "\n",
    "<img src=\"images/activationsReLu.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n",
    "Filters (#filters(size, padding, stride)\n",
    "- 96(11 x 11, 0, 4), 256(5 x 5, 2, 1), 3 x [384(3 x 3, 1, 1)]\n",
    "\n",
    "Pooling\n",
    "- Max-pooling 3x3, stride 2\n",
    "\n",
    "Normalisation layers\n",
    "- normalize the activations of each node by subtracting its mean and dividing by its standard deviation estimating both quantities based on the statistics of the current the current minibatch\n",
    "- typically applied BN after the convolution and before the nonlinear activation function\n",
    "- applied on each channel / feature map \n",
    "\n",
    "<img src=\"images/batchNorm.png\" alt=\"classification\" width=\"400\"/>\n",
    "\n",
    "\n",
    "Dropout layers\n",
    "- Help in removing complex co-adaptations (reducing overfiitting)\n",
    "    * #training samples > 10 * # parameters \n",
    "- net is more robust to noise \n",
    "\n",
    "Loss\n",
    "- Cross-entropy loss\n",
    "\n",
    "#parameters\n",
    "- 60 000 000\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# ZfNet (2013)\n",
    "\n",
    "Parents\n",
    "- Rob Fergus, Matthew D. Zeiler (NYU) -> ClarifAI, CIFAR-10\n",
    "- Zeiler, Matthew D., and Rob Fergus. 2013. “Visualizing and Understanding Convolutional Networks.” CoRR [paper](http://arxiv.org/abs/1311.2901)\n",
    "- Winner of ILSVRC 2013\n",
    "\n",
    "Flow \n",
    "- AlexNet improved based on visualisation of the feature maps\n",
    "\n",
    "<img src=\"images/zfNet.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n",
    "\n",
    "Input\n",
    "- RGB image 224 x 224 x 3\n",
    "\n",
    "Activation\n",
    "- ReLU, tahn\n",
    "\n",
    "Filters (#filters(size, padding, stride)\n",
    "- 96(7 x 7, 0, 2), 256(5 x 5, 2, 1), 512(3 x 3, 1, 1), 1024(3 x 3, 1, 1), 512(3 x 3, 1, 1)\n",
    "\n",
    "Pooling\n",
    "- Max-pooling 3x3, stride 2\n",
    "\n",
    "Loss\n",
    "- Cross-entropy \n",
    "\n",
    "#parameters\n",
    "- TBA\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# VGG (2014)\n",
    "\n",
    "Parents\n",
    "- Simonyan and Zisserman (Visual Geometry Group, Oxford)\n",
    "- Simonyan, Karen, and Andrew Zisserman. 2014. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” [paper](http://arxiv.org/abs/1409.1556.)\n",
    "- Deeper is better!\n",
    "- Winner of ILSVRC 2014 (localisation), 2nd place (classification)\n",
    "\n",
    "Flow \n",
    "- Input -> 2 x \\[Conv -> Pool\\] -> 3 x \\[Conv -> Conv -> Pool\\] -> FC -> FC -> FC -> Softmax -> Output (1000 classes) => VGG-11\n",
    "- Input -> 3 x \\[Conv -> Conv -> Pool\\] -> 2 x \\[Conv -> Conv -> Conv -> Pool\\] -> FC -> FC -> FC -> Softmax -> Output  => VGG-16\n",
    "- Input -> 3 x \\[Conv -> Conv -> Pool\\] -> 2 x \\[Conv -> Conv -> Conv -> Conv -> Pool\\] -> FC -> FC -> FC -> Softmax -> Output => VGG-19\n",
    "\n",
    "<img src=\"images/vgg.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n",
    "Input\n",
    "- RGB image 224 x 224 x 3\n",
    "\n",
    "Activation\n",
    "= ReLU, tahn\n",
    "\n",
    "Blocks = patterns of layers (Conv + ReLU + Pool)\n",
    "- Stacked smaller filters -> complex features learntat a lower cost\n",
    "- \\[Conv64(3 x 3, 1, 1) -> ReLU -> Max-pooling 3x3, stride 2\\]\n",
    "- \\[Conv128(3 x 3, 1, 1) -> ReLU -> Max-pooling 3x3, stride 2\\]\n",
    "- \\[2 x Conv256(3 x 3, 1, 1) -> ReLU -> Max-pooling 2x2, stride 2\\]\n",
    "- \\[2 x Conv512(3 x 3, 1, 1) -> ReLU -> Max-pooling 2x2, stride 2\\]\n",
    "- \\[2 x Conv512(3 x 3, 1, 1) -> ReLU -> Max-pooling 2x2, stride 2\\]\n",
    "\n",
    "Filters \n",
    "- Stacks of smaller filters\n",
    "    * 3 Conv(3x3,0,1) <=> 1 Conv(7 x 7, 0, 1)\n",
    "    * Deeper => more non-linearities \n",
    "    * Fewer parameters (3 * 32 * #Channels2 <=> 1 * 72 * #Channels$^2$)\n",
    "\n",
    "Loss\n",
    "- Cross-entropy loss\n",
    "\n",
    "#parameters\n",
    "- 138 000 000 (VGG16) – a large part of them are used in the final 3 Fully Connected layers\n",
    "- First FC layer -> AvgPooling (InceptionNet, ResNet)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# GoogleLeNet / InceptionNet(2014)\n",
    "\n",
    "Parents\n",
    "- Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. 2016. “Inception-V4, Inception-Resnet and the Impact of Residual Connections on Learning.” [CoRR abs/1602.07261](https://arxiv.org/pdf/1409.4842.pdf)\n",
    "- Inception-V4 [paper](http://arxiv.org/abs/1602.07261)\n",
    "- Top perforamnce ILSVRC 2014 (winner of classification)\n",
    "- movie Inception (“We Need To Go Deeper”)\n",
    "\n",
    "Flow \n",
    "- Input -> Conv -> Pool -> Conv -> Conv -> Pool -> 2 x InceptionBlock -> Pool -> 5 x InceptionBlock -> Poll -> 2 x InceptionBlock -> GlobalPool -> FC -> Output (1000 classes) \n",
    "\n",
    "<img src=\"images/inception.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n",
    "\n",
    "Particularities\n",
    "- FC layers are replaced by AvgPooling => reducing #parameters\n",
    "- Repeatedly usage of the inception block => multiple filters of different sizes.\n",
    "- Naïve Inception block/module <img src=\"images/naiveInceptionModule.png\" alt=\"classification\" width=\"400\"/>\n",
    "    * More Conv Ops:\n",
    "        - [1x1 conv, 128] 28x28x128x1x1x256\n",
    "        - [3x3 conv, 192] 28x28x192x3x3x256\n",
    "        - [5x5 conv, 96] 28x28x96x5x5x256\n",
    "    * Total: 854M ops, 592 K param\n",
    "\n",
    "- Reduced Inception module <img src=\"images/1to1conv.png\" alt=\"classification\" width=\"400\"/>\n",
    "    * Use 1x1 conv to reduce feature depth\n",
    "        - [1x1 conv, 64] 28x28x64x1x1x256\n",
    "        - [1x1 conv, 64] 28x28x64x1x1x256\n",
    "        - [1x1 conv, 128] 28x28x128x1x1x256\n",
    "        - [3x3 conv, 192] 28x28x192x3x3x64\n",
    "        - [5x5 conv, 96] 28x28x96x5x5x64\n",
    "        - [1x1 conv, 64] 28x28x64x1x1x256\n",
    "    * Total: 358M ops, 376 K param\n",
    "\n",
    "- InceptionNet = Stack of Inception module with dimension <img src=\"images/inceptionModule.png\" alt=\"classification\" width=\"400\"/>\n",
    "\n",
    "\n",
    "<img src=\"images/inceptionNet.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n",
    "Input\n",
    "- RGB image 224 x 224 x 3\n",
    "\n",
    "Activation\n",
    "- ReLU, than\n",
    "\n",
    "Loss\n",
    "- Softmax (cross-entropy)\n",
    "\n",
    "#parameters\n",
    "- 5 000 000 parameters (12x less than AlexNet)\n",
    "\n",
    "Versions (v1, v2, v3, v4)\n",
    "- More templates, but the same 3 main properties are kept:\n",
    "- Multiple branches\n",
    "- Shortcuts (1x1, concate.)\n",
    "- Bottleneck\n",
    "\n",
    "<img src=\"images/inceptionVersions.png\" alt=\"classification\" width=\"800\"/>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# ResNet(2014) -  Residual Neural Networks\n",
    "Parents\n",
    "- He et al. (Microsoft)\n",
    "- He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015a. “Deep Residual Learning for Image Recognition.” [CoRR paper](http://arxiv.org/abs/1512.03385)\n",
    "- Winner ILSVRC 2015\n",
    "\n",
    "Flow \n",
    "- 152 layers (following VGG design)\n",
    "\n",
    "<img src=\"images/resnet.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n",
    "\n",
    "Particularities\n",
    "- Batch normalisation after each convolution and before activation \n",
    "- Ultra deep networks with residual connections\n",
    "    * Why?\n",
    "        - error information propagating back tends to get more and more diffused by the time it gets to the initial layers  the weights in the initial few layers are not modified in an optimal fashion => higher errors for deeper nets <img src=\"images/residual.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n",
    "    * How?\n",
    "        - Skip connections <img src=\"images/skipConnections.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n",
    "- A new layer = residual block (skip / shortcut connections) \n",
    "    * <img src=\"images/residualBlock.png\" alt=\"classification\" width=\"400\"/>\n",
    "    * 2 x ( x 3 conv -> batch norm -> ReLU)\n",
    "    * Periodically, double #filters and downsample spatially using stride 2 (/2 in each dimension)\n",
    "    * ResNet ~ ensemble of shallow networks (paths of different lengths)\n",
    "    * Help gradients to penetrate deeper into the network \n",
    "\n",
    "- Batch normalisation after each convolution and before activation <img src=\"images/batchNorm.png\" alt=\"classification\" width=\"400\"/>\n",
    "\n",
    "    * <img src=\"images/batchNormalisation.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n",
    "    * Solves the problem of internal covariate shift (the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change)\n",
    "    * Stabilise and accelerates the convergence\n",
    "    * Group normalisation \n",
    "    * No dropout layers \n",
    "\n",
    "\n",
    "- Large learning rate (initial) 0.1\n",
    "    * Divided by 10 when the validation error plateaus \n",
    "\n",
    "- Additional conv layer at the beginning\n",
    "\n",
    "- No FC layers at the end\n",
    "\n",
    "\n",
    "Input\n",
    "- RGB image 224 x 224 x 3\n",
    "\n",
    "Activation\n",
    "- ReLU, than\n",
    "\n",
    "Loss\n",
    "- Softmax (cross-entropy)\n",
    "\n",
    "#parameters\n",
    "- TBA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# ResNeXt (2017)\n",
    "\n",
    "Hybridisation of Inception and ResNet [paper](https://arxiv.org/pdf/1611.05431.pdf)\n",
    "\n",
    "<img src=\"images/resnext.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n",
    "Particularities\n",
    "- Shortcut\n",
    "- Bottleneck <img src=\"images/uniformMultibranch.png\" alt=\"classification\" width=\"800\"/>\n",
    "- Multi-branch \n",
    "    * concatenation and addition are interchangeable  General property for Deep CNNs\n",
    "    * uniform multi-branching can be done by group-conv\n",
    "\n",
    "<img src=\"images/uniformMultibranchGrouped.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Other architectures\n",
    "\n",
    "- Inception-ResNet (2016)\n",
    "- Dense Net\n",
    "    * Remember Taylor expansions for functions\n",
    "\n",
    "<img src=\"images/denseNet.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n",
    "- Xception, MobileNet (Google)\n",
    "    * Depthwise convolutions (grouped conv with #channels = #group)\n",
    "\n",
    "- others \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Review\n",
    "\n",
    "- Complexity\n",
    "\n",
    "- Forward pass time and power consumtion \n",
    "\n",
    "<img src=\"images/review.png\" alt=\"classification\" width=\"800\"/>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "# Reducing overfitting\n",
    "\n",
    "Possible solutions: \n",
    "- increasing the amount of training data\n",
    "    * Artificially expanding the training data\n",
    "    * Rotations, adding noise, \n",
    "- reduce the size of the network\n",
    "    * Not recommended \n",
    "- regularization techniques \n",
    "    * effects:\n",
    "        - the network prefers to learn small weights, all other things being equal. Large weights will only be allowed if they considerably improve the first part of the cost function\n",
    "        - a way of compromising between finding small weights and minimizing the original cost function (when λ is small we prefer to minimize the original cost function, but when λ is large we prefer small weights)\n",
    "\n",
    "        - Give importance to all features\n",
    "\n",
    "        > $X = [1,1,1,1]$\n",
    "        \n",
    "        > $W_1 = [1, 0, 0, 0]$\n",
    "        \n",
    "        > $W_2 = [0.25, 0.25, 0.25, 0.25]$\n",
    "        \n",
    "        > \n",
    "        \n",
    "        > $W_1^T X  = W_2^T X = 1$\n",
    "        \n",
    "        > $L_1(W_1) = 0.25 + 0.25 + 0.25 + 0.25 = 1$\n",
    "        \n",
    "        > $L_1(W_2) = 1 + 0 + 0 + 0 = 1$\n",
    "\n",
    "    * Methods \n",
    "        - L1 regularisation – add the sum of the absolute values of the weights $C = C_0 +  \\frac{\\lambda}{n} \\sum{|w|}$\n",
    "            * the weights shrink by a constant amount toward 0\n",
    "            * sparsity (feature selection – more weights are 0)\n",
    "        - weight decay (L2 regularization) - add an extra term to the cost function (the L2 regularization term =  the sum of the squares of all the weights in the network = λ/2n ∑w2 ): $C = C_0 +  \\frac{\\lambda}{2n} \\sum{w^2}$ \n",
    "            * the weights shrink by an amount which is proportional to w\n",
    "        - elastic net regularisation\n",
    "            * $\\lambda_1 ∣ w ∣ + \\lambda_2 w^2 \\lambda_1 ∣ w ∣ + \\lambda_2 w^2\n",
    "        - Max norm constraints (clapping)\n",
    "        - Dropout -  modify the network itself (see [link](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf))\n",
    "            * Some neurons are temporarily deleted\n",
    "            * propagate the input and backpropagate the result through the modified network\n",
    "            * update the appropriate weights and biases. \n",
    "            * repeat the process, first restoring the dropout neurons, then choosing a new random subset of hidden neurons to delete\n",
    "\n",
    "- initialisation of weights\n",
    "    * Pitfall: all zero initialization\n",
    "    * Small random numbers \n",
    "    > $W = 0.01 * random(D,H)$\n",
    "    * Calibrating the variances with $\\frac{1}{\\sqrt{\\#Inputs}}$\n",
    "    > $w = random \\frac{\\#Inputs}{\\sqrt{\\#Inputs}}$\n",
    "    * Sparse initialization\n",
    "    * Initializing the biases\n",
    "    * In practice: $w = random(\\#Inputs) * \\sqrt{\\frac{2.0}{\\#Inputs}}$\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Other theoretical details:\n",
    "\n",
    "* Bengio’s [papers](https://arxiv.org/pdf/1206.5533v2.pdf and http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)\n",
    "\n",
    "* Snock’s [paper](http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)\n",
    "\n",
    "* Goodfellow's [chapeter](https://www.deeplearningbook.org/contents/guidelines.html)\n",
    "\n",
    "* Deisenroth's [book](https://d1wqtxts1xzle7.cloudfront.net/61538438/mml-book20191217-47161-13am889.pdf?1576576797=&response-content-disposition=inline%3B+filename%3DMATHEMATICS_FOR_MACHINE_LEARNING.pdf&Expires=1604243049&Signature=e2vXIp11Ww6zcLQtOJ2hypxbwrxR9FWfKa1sPHLoXrP86SJsUyvyU~H2bGFV2Y5sjSGl1IrnU5axULlg9LshykFuQ2EQSj1Cizn6vpd0O-Aoe6~0gSb6Pmv4YkcCUAXPDHrktMwF4p5qmw6g0uiLuhuRl1SKqfgO5L2fge6P0UlaSKrbM5QZ6YQDguz4MW2bhRzrGufIwrpycuSr1lTRTcTmhCRieeNrgtFWKcXuKmBtFIrRoiahT0mZXQzoDaaxdj~U6W6BtXCWguvpBhEH1aZJ6RHsrS~MI2S9Pe6UcSAsNyK6I0We-8qJdod9aXfbNW1wnGky5C4ng5GVaCuw2A__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)\n",
    "\n",
    "* Hand-on ML [book](https://github.com/ageron/handson-ml)\n",
    "\n",
    "\n",
    "Implementations:\n",
    "* [repo](https://github.com/rasbt/deeplearning-models)\n",
    "* pre-trained models \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}